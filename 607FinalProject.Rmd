---
title: "Data 607 Final Project: Movie Review Sentiment Analysis"
author: Valerie Briot, Christophe Hunt, Armenoush Aslanian-Persico
date: May 2016
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: show
    theme: spacelab
---

# Introduction and Motivation

We set out to compare 2016 New York Times movie reviews and tweets related to a given set of movies. 


![](NYTLogo.jpg)

![](twitterlogo.jpg)


<p>

We envisioned a sentiment analysis comparison of the two data sources. How favorably did the New York Times review a given movie compared to the average sentiment on Twitter?

What could this information show us? How could the resulting analysis be helpful to moviegoers and the movie industry?

# Data Science Workflow

We began by identifying our project goals and scope, assessing potential data sources, and retrieving the data.

We first performed an API pull of a list of recent New York Times movie reviews. 

We then performed an API pull of recent tweets. We learned that Twitter provides only a limited search history for publicly available tweets. This resulted in an adjustment of our project scope. 

We then adjusted the date range of our New York Times movie list to contain only the movies with available Twitter data.

We ran a sentiment analysis on the text of the tweets and the text of the movie reviews for the given set of movies.

We created data frames for our data sources, cleaned and transformed the data, and performed analysis on the resulting datasets.

# Libraries

The following libraries were required.

```{r, Libraries, eval=FALSE}
library(rmarkdown)
library(twitteR)
library(httr)
library(ROAuth)
library(jsonlite)
library(knitr)
library(RCurl)
library(XML)
library(indicoio)
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
```

# Sources

## Part 1. Twitter Search Setup

We registered for a Twitter API key to perform the search of public tweets related to movies. 

This code chunk (`Twitter Setup`) was run first as it requires a PIN given by Twitter.  

```{r, Twitter Setup, eval=FALSE}
library(twitteR)
library(httr)
library(ROAuth)

consumer_key <- "consumer_key"
consumer_secret <- "consumer_secret"
access_token <- "access_token"
access_secret <- "access_secret"

download.file(url='http://curl.haxx.se/ca/cacert.pem', destfile='cacert.pem')
reqURL <- 'https://api.twitter.com/oauth/request_token'
accessURL <- 'https://api.twitter.com/oauth/access_token'
authURL <- 'https://api.twitter.com/oauth/authorize'
Cred <- OAuthFactory$new(consumerKey=consumer_key,
                         consumerSecret=consumer_secret,
                         requestURL=reqURL,
                         accessURL=accessURL,
                         authURL=authURL)

Cred$handshake(cainfo = system.file('CurlSSL', 
                                    'cacert.pem', 
                                    package = 'RCurl'))
```

We then ran the following chunks after entering the PIN for Twitter Authorization.

```{r, eval=FALSE}
save(Cred, file='twitter authentication.Rdata')
load('twitter authentication.Rdata') 
setup_twitter_oauth(consumer_key, 
                    consumer_secret, 
                    access_token, 
                    access_secret)
```


## Part 2. NYT Movie Review API

We registered for a New York Times Movie API key to pull the movie review data.

We used the opening date range 2016-04-21 through 2016-05-07.

```{r, eval=TRUE, include=FALSE}
YOURAPIKEYHERE<- "ef056a1d0cdde8b2f03c95bc7dd57722:19:29126896"
```

```{r, eval=FALSE}
#required libraries
library(jsonlite)
library(knitr)

#RESULTS 0-20 AS OF 2016-05-07
NYTREVIEWS_JSON_URL1 = paste0('http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=',YOURAPIKEYHERE, '&order=by-title', collapse = "")#url for api results 0-20
json_file1 <- fromJSON(URLencode(NYTREVIEWS_JSON_URL1))# get json
df1 <- as.data.frame(json_file1$results) #dataframe for results
#str(df1) #view structure 
#colnames(df1) #view column names
df1s<-df1[,c(1:8)] #subset needed columns
#kable(df1s) #test


#RESULTS 21-40 AS OF 2016-05-07
NYTREVIEWS_JSON_URL2 = paste0('http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=', YOURAPIKEYHERE, '&offset=20&order=by-title', collapse = "") #url for api results 20-40
json_file2 <- fromJSON(NYTREVIEWS_JSON_URL2) #get json
df2 <- as.data.frame(json_file2$results) #dataframe for results
df2s<-df2[,c(1:8)] #subset needed columns

#kable(df2s) #test

#test
#NYTREVIEWS_JSON_URL3 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=YOURAPIKEYHERE&offset=40'
#json_file3 <- fromJSON(NYTREVIEWS_JSON_URL3) #get json
#df3 <- as.data.frame(json_file3$results) #dataframe for results
#df3s<-df3[,c(1:8)] #subset needed columns
#kable(df3s)

#ALL RESULTS 0-40 COMBINED
df<- rbind(df1s,df2s) #combine all results
#knitr::kable(df, row.names =TRUE) #test

#ADD URL COLUMN TO RESULTS
url1 <- json_file1$results$link #collect links part 1
url1<-url1[,c(1:2)] #subset links
url2 <- json_file2$results$link #collect links part 2
url2<-url2[,c(1:2)] #subset links
#str(url1) #test
urls<- rbind(url1,url2) #combine all results for links
#knitr::kable(urls, row.names = TRUE) #test
combo<-cbind(df,urls) #combine reviews and links
combo<-combo[,c(1:3,7,8,10)] #subset needed columns
combo<-combo[,c(1,5,4,6,2,3)] #reorder needed columns
kable(combo, row.names = TRUE) #display with row numbers
```


Exported to CSV.

```{r, eval=FALSE}
write.table(combo, "nytmovies.csv") 
```

The resulting CSV is available here:

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/nytmovies.csv">List of NYT Movie Reviews</a>

## Part 3. NYT Sentiment Analysis

For each URL pointing to an actual movie review, we then scraped the text of the review. Only the actual text of the review was considered. This was indicated by the class = 'story-body-text story-content' under the paragraph tag "< p >".  

We then ran the review text through sentiment analysis with indico, again resulting in a 0-1 score.


```{r, eval = FALSE}
# For each URl in combo

library(RCurl)
library(XML)
library(indicoio)


movie_score <- data.frame()

for (i in 1:length(combo$url)){
  movie_url <- combo$url[i] 
  my_movie <- readLines(movie_url, encoding = "UTF-8")
  content_review <- htmlTreeParse(my_movie, asText=TRUE, useInternalNodes = TRUE, encoding = 'UTF-8')
  plain.text <- xpathSApply(xmlRoot(content_review), "//p[@class = 'story-body-text story-content']", xmlValue)
  plain.text <- as.data.frame(as.character(paste(plain.text, collapse = " ")))
  colnames(plain.text) <- "review"
  plain.text$movie <- combo$display_title[i]
  plain.text$Sentiment_score <- unlist(sentiment(as.character(rm_special(plain.text$review)),api_key = indico_api_key))
  movie_score <- rbind(movie_score, plain.text)
}

combo <- cbind(combo, movie_score)
```


```{r, include= FALSE, eval=FALSE}
url <- "https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/"
```

```{r, eval= FALSE}
data.url <- file(paste(url,"locations.csv", sep = ""), open="r" )
locations <- read.csv(data.url, sep=",", header=TRUE, stringsAsFactors = FALSE)
```

> The resulting CSV is available here:
/////


## Part 4. Twitter Sentiment Analysis

Once we established the list of available New York Times movie reviews, we ran the Twitter search using the movie title as a search term. To get better Twitter search results for one-word movies, we added the word "movie" to our search term.

This chunk retrieved the Twitter search results for a term. `searchTwitter` has some other settings that could probably improve results.

We used the indico package to perform sentiment analysis, resulting in a score of 0-1 for each tweet, 1 being extremely positive.

We decided to use unique tweets only, therefore eliminating any retweet data.

For the benefit of our analysis, we filtered the Twitter search results to pull only the tweets which were coded with geographical location.

We chose the following cities at random: 

```{r, eval=FALSE}
paste(locations$City, collapse = ", ")
```

```{r, eval=TRUE}
library(knitr)
library(RCurl)
citiesurl <- getURL("https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/locations.csv")

citiesxy<- data.frame(read.csv(text=citiesurl))

kable(citiesxy)
```

```{r, Using Movies CSVs, include = FALSE, eval=FALSE}

movie_titles_1 <- read.csv(file(paste(url,"20160508nytreviews.csv", sep = ""), open="r" ), sep=" ", header=TRUE, stringsAsFactors = FALSE)

movie_titles_2 <- read.csv(file(paste(url,"20160501nytreviews.csv", sep = ""), open="r" ), sep=" ", header=TRUE, stringsAsFactors = FALSE)

movies <- as.data.frame(unique(
  c(enc2utf8(gsub("â???T", "'",as.character(movie_titles_1$display_title))),
                                            enc2utf8(gsub("â???T", "'",as.character(movie_titles_2$display_title))))))
colnames(movies) <- "display_titles"
```

```{r, Searching Twitter, eval= FALSE}
library(indicoio)

rm_special <-function(x) iconv(x, "UTF-8", "UTF-8",sub='')

search_term <- function(term){
                  require(magrittr)
                  search_term <- gsub(" ", "+", term) %>%
                                 paste("+movie", sep = "")
                  return(search_term)
                }


df <- NULL
for (i in movies$display_titles){
  SearchTerm <- search_term(i)
  for (j in seq(1:length(locations$City))){
      list <- suppressWarnings(searchTwitter(SearchTerm,  
                                             since = format(Sys.Date()-1,format="%Y-%m-%d"), 
                                             until = format(Sys.Date(),format="%Y-%m-%d"), n = 5000, 
                                    geocode = paste(locations$Lat[j], ",", locations$Long[j], ",30mi", sep = "" )))
          if (length(list) == 0){ 
            NULL
          } else { 
          tweetdf <- twListToDF(list)
          tweetdf <- as.data.frame(unique(tweetdf$text))
          colnames(tweetdf) <- c("tweet")
          tweetdf$city <- locations$City[j]
          tweetdf$movie <- as.character(i)
          tweetdf$day <- format(Sys.Date() ,format="%m.%d.%Y")
          df <- rbind(df,tweetdf)
          }
         } 
       }

df$Sentiment_Score <- unlist(sentiment(as.character(rm_special(df$tweet)),  api_key = indico_api_key))
write.csv(df, file = paste("Tweets for ", as.character(format(Sys.Date(), format="%m.%d.%Y")), ".csv", sep = ""))
write.csv(combo$display_title, file = "movie titles used.csv")
```

The resulting Twitter CSVs for individual days are available here: 

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.03.2016.csv">Tweets for 20160503</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.04.2016.csv">Tweets for 20160504</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.05.2016.csv">Tweets for 20160505</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.06.2016.csv">Tweets for 20160506</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.07.2016.csv">Tweets for 20160507</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.08.2016.csv">Tweets for 20160508</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.09.2016.csv">Tweets for 20160509</a>


# Data Transformation and Analysis

For our primary twitter dataset, we combined all seven days of tweets into one dataframe of 5,445 records.

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/tweets503to509.csv">Complete Dataset of All Tweets With Sentiment Score</a>

We created a main dataframe to include basic movie information, NYT Critics' Pick score, NYT Review Sentiment Score, Twitter Average Sentiment Score, and Count of Tweets. 

We analyzed each day's worth of tweets separately to identify outliers and possible trends. The complete analysis is available here.

<a href="http://rpubs.com/aapsps/179750">Analysis of Movie Tweets By Day</a>

We then analyzed the complete dataset of tweets.

## Movie Score
```{r, eval=TRUE, include=FALSE}
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(RCurl)
```

```{r, eval=FALSE, include=FALSE}
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(RCurl)
```


```{r, eval=TRUE}

tweets1 <- getURL("https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/tweets503to509.csv")

tweets1<- data.frame(read.csv(text=tweets1))

moviesum1<- ddply(tweets1, .(movie), summarize,  Sentiment_Score=mean(Sentiment_Score), Count_of_Tweets=length(tweet)) #summarize by movie

moviesum1$movie<-as.character(moviesum1$movie) #convert levels to character
moviesum1<- arrange(moviesum1, movie) #sort alphabetically by movie name

moviesum1$num<-seq.int(nrow(moviesum1)) # add counter row

kable(moviesum1)
```

## City Score

```{r, eval=TRUE}
citysum1<- ddply(tweets1, .(city), summarize,  Sentiment_Score=mean(Sentiment_Score), Count_of_Tweets=length(tweet)) #summarize by city
kable(citysum1)
```

## Movie and City Score

```{r, eval=TRUE}
moviecitysum1<- ddply(tweets1, .(movie, city), summarize,  Sentiment_Score=mean(Sentiment_Score)) #summarize by movie and city

kable(moviecitysum1)
```

## Graphs

```{r, eval=TRUE}
ggplot(
  moviesum1, aes(x = reorder(movie,Sentiment_Score), y = Sentiment_Score, fill=Sentiment_Score)) + 
  geom_bar(stat="identity") +
  ggtitle("Movie Average Sentiment Score")+ 
  theme(axis.text=element_text(angle=90))+
  labs(x="Movies",y="Score")

ggplot(
  citysum1, aes(x = reorder(city,Sentiment_Score), y = Sentiment_Score, fill=Sentiment_Score)) + 
  geom_bar(stat="identity") +
  ggtitle("City Average Sentiment Score")+ 
  theme(axis.text=element_text(angle=90))+
  labs(x="City",y="Score")
```


# New Features

Twitter API / OAUTH

Testing different sentiment analysis machines

Indico package

Collaborating on Slack

Collaborating on Github 

# Challenges

Choosing tweets

Retweets and removing duplicates

Movie titles with common words

# Insights and Conclusion
