---
title: "Data 607 Final Project: Movie Review Sentiment Analysis"
author: Valerie Briot, Christophe Hunt, Armenoush Aslanian-Persico
date: May 2016
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: show
    theme: spacelab
---

# Introduction and Motivation

We set out to compare 2016 New York Times movie reviews and tweets related to a given set of movies. 


<center><img src="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/NYTLogo.jpg" width="500px" /></center>

<p>

<center><img src="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/twitterlogo.jpg" width="500px" /></center>

<p>

<center><img src="https://indico.io/static/img/logo-black.png" width="500px" /></center>

<p>

We envisioned a sentiment analysis comparison of the two data sources. How favorably did the New York Times review a given movie compared to the average sentiment on Twitter?

What could this information show us? How could the resulting analysis be helpful to moviegoers and the movie industry?

# Data Science Workflow

We began by identifying our project goals and scope, assessing potential data sources, and retrieving the data.

We first performed an API pull of a list of recent New York Times movie reviews. 

We then performed an API pull of recent tweets. We learned that Twitter provides only a limited search history for publicly available tweets. This resulted in an adjustment of our project scope. 

We then adjusted the date range of our New York Times movie list to contain only the movies with available Twitter data.

We ran a sentiment analysis on the text of the tweets and the text of the movie reviews for the given set of movies.

We created data frames for our data sources, cleaned and transformed the data, and performed analysis on the resulting datasets.

<center><img src="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Diagram.PNG" height="800px" /></center>

# Libraries

The following libraries were required.

```{r, Libraries, eval=TRUE, warning=FALSE, message=FALSE}

#library(indicoio)
library(ggplot2)
library(httr)
library(jsonlite)
library(knitr)
library(leaflet)
library(plotly)
library(plyr)
library(dplyr)
library(RCurl)
library(rmarkdown)
library(ROAuth)
library(stringr)
library(tidyr)
library(twitteR)
library(XML)

```

# Sources

## Part 1. Twitter Search Setup

We registered for a Twitter API key to perform the search of public tweets related to movies. 

This code chunk (`Twitter Setup`) was run first as it requires a PIN given by Twitter.  

```{r, Twitter Setup, eval=FALSE}
library(twitteR)
library(httr)
library(ROAuth)

consumer_key <- "consumer_key"
consumer_secret <- "consumer_secret"
access_token <- "access_token"
access_secret <- "access_secret"

download.file(url='http://curl.haxx.se/ca/cacert.pem', destfile='cacert.pem')
reqURL <- 'https://api.twitter.com/oauth/request_token'
accessURL <- 'https://api.twitter.com/oauth/access_token'
authURL <- 'https://api.twitter.com/oauth/authorize'
Cred <- OAuthFactory$new(consumerKey=consumer_key,
                         consumerSecret=consumer_secret,
                         requestURL=reqURL,
                         accessURL=accessURL,
                         authURL=authURL)

Cred$handshake(cainfo = system.file('CurlSSL', 
                                    'cacert.pem', 
                                    package = 'RCurl'))
```

We then ran the following chunks after entering the PIN for Twitter Authorization.

```{r, eval=FALSE}
save(Cred, file='twitter authentication.Rdata')
load('twitter authentication.Rdata') 
setup_twitter_oauth(consumer_key, 
                    consumer_secret, 
                    access_token, 
                    access_secret)
```


## Part 2. NYT Movie Review API

We registered for a New York Times Movie API key to pull the movie review data.

We used the opening date range 2016-04-21 through 2016-05-07.

```{r, eval=TRUE, include=FALSE}
YOURAPIKEYHERE<- "ef056a1d0cdde8b2f03c95bc7dd57722:19:29126896"
```

```{r, eval=FALSE}
#required libraries
library(jsonlite)
library(knitr)

#RESULTS 0-20 AS OF 2016-05-07
NYTREVIEWS_JSON_URL1 = paste0('http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=',YOURAPIKEYHERE, '&order=by-title', collapse = "")#url for api results 0-20
json_file1 <- fromJSON(URLencode(NYTREVIEWS_JSON_URL1))# get json
df1 <- as.data.frame(json_file1$results) #dataframe for results
#str(df1) #view structure 
#colnames(df1) #view column names
df1s<-df1[,c(1:8)] #subset needed columns
#kable(df1s) #test


#RESULTS 21-40 AS OF 2016-05-07
NYTREVIEWS_JSON_URL2 = paste0('http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=', YOURAPIKEYHERE, '&offset=20&order=by-title', collapse = "") #url for api results 20-40
json_file2 <- fromJSON(NYTREVIEWS_JSON_URL2) #get json
df2 <- as.data.frame(json_file2$results) #dataframe for results
df2s<-df2[,c(1:8)] #subset needed columns

#kable(df2s) #test

#test
#NYTREVIEWS_JSON_URL3 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-07&api-key=YOURAPIKEYHERE&offset=40'
#json_file3 <- fromJSON(NYTREVIEWS_JSON_URL3) #get json
#df3 <- as.data.frame(json_file3$results) #dataframe for results
#df3s<-df3[,c(1:8)] #subset needed columns
#kable(df3s)

#ALL RESULTS 0-40 COMBINED
df<- rbind(df1s,df2s) #combine all results
#knitr::kable(df, row.names =TRUE) #test

#ADD URL COLUMN TO RESULTS
url1 <- json_file1$results$link #collect links part 1
url1<-url1[,c(1:2)] #subset links
url2 <- json_file2$results$link #collect links part 2
url2<-url2[,c(1:2)] #subset links
#str(url1) #test
urls<- rbind(url1,url2) #combine all results for links
#knitr::kable(urls, row.names = TRUE) #test
combo<-cbind(df,urls) #combine reviews and links
combo<-combo[,c(1:3,7,8,10)] #subset needed columns
combo<-combo[,c(1,5,4,6,2,3)] #reorder needed columns
kable(combo, row.names = TRUE) #display with row numbers
write.table(combo, "nytmovies.csv") #export to CSV
```

The resulting CSV is available here:

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/nytmovies.csv">List of NYT Movie Reviews links</a>

## Part 3. NYT Sentiment Analysis

###Data Retrieval and Sentiment Analysis  

For each URL pointing to an actual movie review, we then scraped the text of the review. Only the actual text of the review was considered. This was indicated by the class = 'story-body-text story-content' under the paragraph tag "< p >".  In addition, we will retrieve the genre of the movie listed in the individual review.  The Genres are indicated by the following tag <span itemprop='genre' class='genre>.  We will only retrieve the first genre tag. 

We then ran the review text through sentiment analysis with indico, again resulting in a 0-1 score.

For reproducibility of results, we read the reviews listing and review links from github.

```{r, eval = FALSE}
# Retrieve list of movie from Github
movie_list_url <- getURL("https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/nytmovies2.csv")
movie_list <- read.csv(textConnection(movie_list_url), header = TRUE, sep = ",")

movie_list[1:5,]
str(movie_list)


# initialize resulting data frame
movie_score_nyt <- data.frame()

# initialize variables:

# INDICO API Key 
my_indicoio_API_Key <-"2cf9508e4628b67c9b69ae7d1059efda"

# Set Up xpath to retrieve review and genre
xpath_review <- "//p[@class = 'story-body-text story-content']"
xpath_genre <- "(//span[@itemprop='genre'][@class = 'genre'])[1]" # will only retrieve first one

agent <- c(R.version$version.string, ",", R.version$platform)

# Loop through movie list, retrieve review for each movie review url
loop_counter <- nrow(movie_list)

for (i in 1:loop_counter){
  
  #Set RCurl pars
  curl <- getCurlHandle()
  curlSetOpt(cookiejar="cookies.txt",  useragent = agent, followlocation = TRUE, curl=curl)
  
  movie_url <-  movie_list$url[i]
  
  # Get Review corresponding the url
  my_movie <- getURL(as.character(movie_url), curl = curl, verbose = FALSE)
  
  content_review = htmlTreeParse(my_movie, asText=TRUE, useInternalNodes = TRUE, encoding = 'UTF-8')
  
  plain_text <- xpathSApply(xmlRoot(content_review), xpath_review, xmlValue)
  review_df <- as.data.frame(as.character(paste(plain_text, collapse = " ")))
  
  colnames(review_df) <- "review"
  review_df$movie <- movie_list$movie[i]
  
  review_df$Sentiment_Score_nyt <- unlist(sentiment(as.character(review_df$review), api_key = my_indicoio_API_Key))
  
  genre <- xpathSApply(xmlRoot(content_review), xpath_genre, xmlValue)
  review_df$genre <-genre
  
  movie_score <- rbind(movie_score, review_df)
}

movie_score_nyt <- inner_join(movie_list, movie_score, by = "movie")


write.csv(movie_score_nyt, file = "movie_df.csv", sep = ",")

```

The full dataframe for NYT movie reviews can be found on GitHub at:  
[https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/movie_score_nyt.csv](movie_score_nyt.csv)

###Sentiment Analysis litmus test  
How did the sentiment analysis for NY Times reviews performed.  The NY Times does not provide "stars" to rate their movie reviews but favorable reviews will be tagged with a "Critics' Pick".  As a test for the sentiment analysis accuracy, we will analyse the results as follows:  



```{r, include= FALSE, eval=TRUE}
url <- "https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/"
```

```{r, eval= TRUE}
data.url <- file(paste(url,"locations.csv", sep = ""), open="r" )
locations <- read.csv(data.url, sep=",", header=TRUE, stringsAsFactors = FALSE)
```


```{r, eval= TRUE}

url <- "https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/"

data.url <- file(paste(url,"movie_score_nyt.csv", sep = ""), open="r" )
reviewscore <- read.csv(data.url, sep=",", header=TRUE, stringsAsFactors = FALSE)

reviewscore<-reviewscore[,c(2:8,10:11)] #subset needed columns
kable(reviewscore)
```

## Part 4. Twitter Sentiment Analysis

Once we established the list of available New York Times movie reviews, we ran the Twitter search using the movie title as a search term. To get better Twitter search results for one-word movies, we added the word "movie" to our search term.

This chunk retrieved the Twitter search results for a term. `searchTwitter` has some other settings that could probably improve results.

We used the indico package to perform sentiment analysis, resulting in a score of 0-1 for each tweet, 1 being extremely positive.

We decided to use unique tweets only, therefore eliminating any retweet data.

For the benefit of our analysis, we filtered the Twitter search results to pull only the tweets which were coded with geographical location.

We chose the following cities at random: 
'r paste(str_trim(locations$City), collapse = ", ")`.

```{r, eval=TRUE}
library(leaflet)
leaflet(locations) %>% 
  addProviderTiles("Stamen.Toner") %>% 
  addMarkers(lng = ~Long, lat = ~Lat, popup = paste(as.character(str_trim(locations$City))))
```

```{r, eval=TRUE}
library(knitr)
library(RCurl)
citiesurl <- getURL("https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/locations.csv")

citiesxy<- data.frame(read.csv(text=citiesurl))

kable(citiesxy)
```

```{r, Using Movies CSVs, include = FALSE, eval=true}

movie_titles_1 <- read.csv(file(paste(url,"20160508nytreviews.csv", sep = ""), open="r" ), sep=" ", header=TRUE, stringsAsFactors = FALSE)

movie_titles_2 <- read.csv(file(paste(url,"20160501nytreviews.csv", sep = ""), open="r" ), sep=" ", header=TRUE, stringsAsFactors = FALSE)

movies <- as.data.frame(unique(
  c(enc2utf8(gsub("â???T", "'",as.character(movie_titles_1$display_title))),
                                          enc2utf8(gsub("â???T", "'",as.character(movie_titles_2$display_title))))))
colnames(movies) <- "display_titles"

```

```{r, Searching Twitter, eval= FALSE}
library(indicoio)

rm_special <-function(x) iconv(x, "UTF-8", "UTF-8",sub='')

search_term <- function(term){
                  require(magrittr)
                  search_term <- gsub(" ", "+", term) %>%
                                 paste("+movie", sep = "")
                  return(search_term)
                }


df <- NULL
for (i in movies$display_titles){
  SearchTerm <- search_term(i)
  for (j in seq(1:length(locations$City))){
      list <- suppressWarnings(searchTwitter(SearchTerm,  
                                             since = format(Sys.Date()-1,format="%Y-%m-%d"), 
                                             until = format(Sys.Date(),format="%Y-%m-%d"), n = 5000,
                                             geocode = paste(locations$Lat[j], ",", locations$Long[j], ",30mi", sep = "" )
                                             ))
          if (length(list) == 0){ 
            NULL
          } else { 
          tweetdf <- twListToDF(list)
          tweetdf <- as.data.frame(unique(tweetdf$text))
          colnames(tweetdf) <- c("tweet")
          tweetdf$city <- locations$City[j]
          tweetdf$movie <- as.character(i)
          tweetdf$day <- format(Sys.Date() ,format="%m.%d.%Y")
          df <- rbind(df,tweetdf)
          }
         } 
       }

df$Sentiment_Score <- unlist(sentiment(as.character(rm_special(df$tweet)),  api_key = indico_api_key))
write.csv(df, file = paste("Tweets for ", as.character(format(Sys.Date(), format="%m.%d.%Y")), ".csv", sep = ""))
```

The resulting Twitter CSVs for individual days are available here: 

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.03.2016.csv">Tweets for 20160503</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.04.2016.csv">Tweets for 20160504</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.05.2016.csv">Tweets for 20160505</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.06.2016.csv">Tweets for 20160506</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.07.2016.csv">Tweets for 20160507</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.08.2016.csv">Tweets for 20160508</a>

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.09.2016.csv">Tweets for 20160509</a>


# Data Transformation and Analysis

For our primary twitter dataset, we combined all seven days of tweets into one dataframe of 5,445 records.

<a href="https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/tweets503to509.csv">Complete Dataset of All Tweets With Sentiment Score</a>

We created a main dataframe to include basic movie information, NYT Critics' Pick score, NYT Review Sentiment Score, Twitter Average Sentiment Score, and Count of Tweets. 

We analyzed each day's worth of tweets separately to identify outliers and possible trends. The complete analysis is available here.

<a href="http://rpubs.com/aapsps/179750">Analysis of Movie Tweets By Day</a>

We then analyzed the complete dataset of tweets.

## Movie Score
```{r, eval=TRUE, include=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(plyr)
library(dplyr)
library(ggplot2)
library(knitr)
library(RCurl)
```


```{r, eval=TRUE}

tweets1 <- getURL("https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/tweets503to509.csv")

tweets1<- data.frame(read.csv(text=tweets1))
```

```{r, exceptions, eval = TRUE}
tweets1$city[tweets1$city == 'los Angeles'] <- 'Los Angeles'
tweets1$movie <- encodeString(as.character(tweets1$movie))
tweets1$movie[tweets1$movie == 'Mother<U+0092>s Day'] <- "Mother's Day"
tweets1$movie[tweets1$movie == "L<U+0092>Attesa (The Wait)"] <- "L'Attesa (The Wait)"
```

```{r, eval=TRUE}
moviesum1<- ddply(tweets1, .(movie), summarize,  Sentiment_Score=mean(Sentiment_Score), Count_of_Tweets=length(tweet)) #summarize by movie

moviesum1$movie<-as.character(moviesum1$movie) #convert levels to character
moviesum1<- arrange(moviesum1, movie) #sort alphabetically by movie name
moviesum1$num<-seq.int(nrow(moviesum1)) # add counter row

kable(moviesum1)
```

## City Score

```{r, eval=TRUE}
citysum1<- ddply(tweets1, .(city), summarize,  Sentiment_Score=mean(Sentiment_Score), Count_of_Tweets=length(tweet)) #summarize by city
kable(citysum1)
```

## Movie and City Score

```{r, eval=FALSE}
moviecitysum1<- ddply(tweets1, .(movie, city), summarize,  Sentiment_Score=mean(Sentiment_Score)) #summarize by movie and city
kable(moviecitysum1)
```

# Visualization

## Bar Plots

```{r, eval=TRUE}

ggplot(
  moviesum1, aes(x = reorder(movie,Sentiment_Score), y = Sentiment_Score, fill=Sentiment_Score)) + 
  geom_bar(stat="identity") +
  ggtitle("Movie Average Sentiment Score")+ 
  theme(axis.text=element_text(angle=90))+
  labs(x="Movies",y="Score")

ggplot(
  citysum1, aes(x = reorder(city,Sentiment_Score), y = Sentiment_Score, fill=Sentiment_Score)) + 
  geom_bar(stat="identity") +
  ggtitle("City Average Sentiment Score")+ 
  theme(axis.text=element_text(angle=90))+
  labs(x="City",y="Score")
```

## Box Plots

```{r, eval=TRUE, warning=FALSE, message=FALSE, height=800, width = 600}
library(plotly)

top_movies <- ddply(tweets1, ~movie, summarize, tweet_count = length(tweet))
top_movies <- top_movies[order(-top_movies$tweet_count),]
n <- 10
top_movies <- head(top_movies$movie, n = n)
data.url <- file(paste(url,"movie_score.csv", sep = ""), open="r" )
movies_score_NYT <- read.csv(data.url, sep=",", header=TRUE, stringsAsFactors = FALSE)
movies_score_NYT$movie[movies_score_NYT$X == 22] <- "Mother's Day"
movies_score_NYT_subset <- subset(movies_score_NYT, movies_score_NYT$movie %in% top_movies)

a <- list()
for (i in seq_len(nrow(movies_score_NYT_subset ))) {
  m <- movies_score_NYT_subset [i, ]
  a[[i]] <- list(
  x = movies_score_NYT_subset$movie[i],
  y = movies_score_NYT_subset$Sentiment_score[i],
  text = paste("NYT Score: ", signif(movies_score_NYT_subset$Sentiment_score[i],5)),
  xref = "x", yref = "y", showarrow = TRUE,  arrowhead = 7,  ax = 10,  ay = -20
  )
}

m = list( l = 50,  r = 50,  b = 150,  t = 100,  pad = 5)

xlabel <- list(title = "Movie")
ylabel <- list(title = "Setiment Score")

plot_ly(subset(tweets1, tweets1$movie %in% top_movies), 
        y = Sentiment_Score, 
        color = movie, 
        type = "box", 
        boxpoints = "all", jitter = 0.3) %>%
  layout(title = sprintf("Box Plot of Movie Tweets for top %s movies", n),
         xaxis = xlabel, 
         yaxis = ylabel,
         annotations = a,
         autosize = F, width = 1000, height = 800, margin = m) 

```

<br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br><br>

```{r}
library(RColorBrewer)

mapped_tweets <- merge(tweets1, locations, all.x = TRUE, by.y = "City", by.x = "city")  %>%
                group_by(movie, city, Long, Lat) %>%
                summarise( score = mean(Sentiment_Score))

mapped_tweets_top <- subset(mapped_tweets, mapped_tweets$movie %in% top_movies)

factpal <- colorFactor(brewer.pal(10, "Spectral"), mapped_tweets_top$movie)

leaflet(mapped_tweets_top) %>% 
  addProviderTiles("Stamen.Toner") %>%
  addCircles(lng = ~jitter(Long, factor = 20), lat = ~jitter(Lat, factor = 20), weight = 2,
    radius = ~score*100000, 
    popup =paste("<font color=", factpal(mapped_tweets_top$movie), ">" , 
                 "Movie:", as.character(mapped_tweets_top$movie), "<br>", "Sentiment Score:", 
                           as.character(signif(mapped_tweets_top$score,3)), "</font>"),
    color = ~factpal(mapped_tweets_top$movie),
    fillOpacity = 0.4)

```

# New Features

## Twitter API / OAUTH

We utilized the Twitter API to develop our collection of tweets. Using the `twitteR` package we were able to pull tweets for locations by setting the geocode location and radius. OAuth was necessary because the `twitteR` uses the Developer API to search tweets based on our settings, it also required interaction by allowing the application access by putting in a key given by Twitter. 

## Sentiment analysis machines

We tested a number of different sentiment analysis machines.

We started out trying to using the bag of words method to count positive and negative words within the tweet to get the positivity score. The problem with this method was that many of the words we had did not show up in tweets and the context of the tweet was typically misclassified. 

We realized that we needed to get some help to get a more accurate sentiment analysis. We looked into several APIs such as IBM's [AlchemyAPI](http://www.alchemyapi.com/api/sentiment-analysis). We ended up using Indico since it offered a free tier and they had developed an easy to use `R` package. 

## Indico package

We used the Indico Sentiment Analysis API to analyze our text to return the positity on a scale from 0-1. We tested out several obviously bad and positive tweets and the response was very accurate. 

 > Indico Benchmark: The API performs with 93% accuracy on the IMDB dataset (state of the art).

Collaborating on Slack


Collaborating on Github 

# Challenges

Choosing tweets

Retweets and removing duplicates

Movie titles with common words

# Insights and Conclusion

Twitter has a limitation on interpretability, it is useful as a proof of concept but cannot be the sole source of information. Many tweets were commerical advertisements and had unhelpful links. Twitter is a useful tool to explore but I would not build a data product exclusively with it. 

