---
title: "Data 607 Final Project: Movie Review Sentiment Analysis"
author: Valerie Briot, Christophe Hunt, Armenoush Aslanian-Persico
date: May 2016
output: 
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    code_folding: hide
---

# Introduction and Motivation

We set out to compare 2016 New York Times movie reviews and tweets related to a given set of movies. 

We envisioned a sentiment analysis comparison of the two data sources. How favorably did the New York Times review a given movie compared to the average sentiment on Twitter?

What could this information show us? How could the resulting analysis be helpful to moviegoers and the movie industry?

# Data Science Workflow

We began with an API pull of a list of recent New York Times movie reviews. 

We then performed an API pull of recent tweets. We learned that Twitter provides only a limited search history for publicly available tweets. This resulted in an adjustment of our project scope. 

We then adjusted the date range of our New York Times movie list to contain only the movies with available Twitter data. 

We ran a sentiment analysis on the text of the tweets and the text of the movie reviews for the given set of movies.

We created data frames for our data sources, cleaned and trasformed the data, and performed analysis on the resulting datasets.

# Sources

## Part 1. Initiate Twitter Search

We registered for a Twitter API key to perform the search of public tweets related to movies. 

This code chunk (`Twitter Setup`) was run first as it requires a PIN given by Twitter.  

```{r, Twitter Setup, eval=FALSE}
library(twitteR)
library(httr)
library(ROAuth)

consumer_key <- "needs_keys_from_slack"
consumer_secret <- "needs_keys_from_slack"
access_token <- "needs_keys_from_slack"
access_secret <- "needs_keys_from_slack"

download.file(url='http://curl.haxx.se/ca/cacert.pem', destfile='cacert.pem')
reqURL <- 'https://api.twitter.com/oauth/request_token'
accessURL <- 'https://api.twitter.com/oauth/access_token'
authURL <- 'https://api.twitter.com/oauth/authorize'
Cred <- OAuthFactory$new(consumerKey=consumer_key,
                         consumerSecret=consumer_secret,
                         requestURL=reqURL,
                         accessURL=accessURL,
                         authURL=authURL)

Cred$handshake(cainfo = system.file('CurlSSL', 
                                    'cacert.pem', 
                                    package = 'RCurl'))
```

We then ran the following chunks after entering the PIN for Twitter Authorization.

```{r, eval=FALSE}
save(Cred, file='twitter authentication.Rdata')
load('twitter authentication.Rdata') 
setup_twitter_oauth(consumer_key, 
                    consumer_secret, 
                    access_token, 
                    access_secret)
```


## Part 2. Retrieve API of New York Times Movie Reviews

We registered for a New York Times Movie API key to pull the movie review data.

We used the opening date range 2016-04-21 through 2016-05-06.


```{r, eval=FALSE}
#This code chunk is for display only and does not contain the actual API key.

#required libraries
library(jsonlite)
library(knitr)

#RESULTS 0-20 AS OF 2016-05-01
NYTREVIEWS_JSON_URL1 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-018&api-key=YOURAPIKEYHERE'#url for api results 0-20
json_file1 <- fromJSON(NYTREVIEWS_JSON_URL1)# get json
df1 <- as.data.frame(json_file1$results) #dataframe for results
#str(df1) #view structure 
#colnames(df1) #view column names
df1s<-df1[,c(1:8)] #subset needed columns

#RESULTS 21-40 AS OF 2016-05-01
NYTREVIEWS_JSON_URL2 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-01&api-key=YOURAPIKEYHERE&offset=20' #url for api results 20-40
json_file2 <- fromJSON(NYTREVIEWS_JSON_URL2) #get json
df2 <- as.data.frame(json_file2$results) #dataframe for results
df2s<-df2[,c(1:8)] #subset needed columns

#ALL RESULTS 0-40 COMBINED
df<- rbind(df1s,df2s) #combine all results
#knitr::kable(df, row.names =TRUE) #test

#ADD URL COLUMN TO RESULTS
url1 <- json_file1$results$link #collect links part 1
url1<-url1[,c(1:2)] #subset links
url2 <- json_file2$results$link #collect links part 2
url2<-url2[,c(1:2)] #subset links
#str(url1) #test
urls<- rbind(url1,url2) #combine all results for links
#knitr::kable(urls, row.names = TRUE) #test
combo<-cbind(df,urls) #combine reviews and links
combo<-combo[,c(1:3,7,8,10)] #subset needed columns
combo<-combo[,c(1,5,4,6,2,3)] #reorder needed columns
kable(combo, row.names = TRUE) #display with row numbers
```


```{r, echo=FALSE, eval=FALSE}
# This code chunk contains the actual API key and needs to be set to eval=TRUE to generate the CSV.

library(jsonlite)
library(knitr)

#RESULTS 0-20 AS OF 2016-05-01

NYTREVIEWS_JSON_URL1 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?openingmurl1-date=2016-04-21;2016-05-018&api-key=ef056a1d0cdde8b2f03c95bc7dd57722:19:29126896'
json_file1 <- fromJSON(NYTREVIEWS_JSON_URL1)
df1 <- as.data.frame(json_file1$results)
#str(df1)
#colnames(df1)
df1s<-df1[,c(1:8)]


#RESULTS 21-40 AS OF 2016-05-01

NYTREVIEWS_JSON_URL2 = 'http://api.nytimes.com/svc/movies/v2/reviews/search.json?opening-date=2016-04-21;2016-05-01&api-key=ef056a1d0cdde8b2f03c95bc7dd57722:19:29126896&offset=20'
json_file2 <- fromJSON(NYTREVIEWS_JSON_URL2)
df2 <- as.data.frame(json_file2$results)
df2s<-df2[,c(1:8)]


df<- rbind(df1s,df2s)
#knitr::kable(df, row.names =TRUE)



url1 <- json_file1$results$link
url1<-url1[,c(1:2)]

url2 <- json_file2$results$link
url2<-url2[,c(1:2)]

#str(url1)
urls<- rbind(url1,url2)
#knitr::kable(urls, row.names = TRUE)

combo<-cbind(df,urls)
combo<-combo[,c(1:3,7,8,10)]
combo<-combo[,c(1,5,4,6,2,3)]
kable(combo, row.names = TRUE)

write.table(combo, "20160501nytreviews.csv") 
```

## Part 3 Retrieve NY Times review text for each entry and perform sentiment analysis

For each URL pointing to an actual movie review, we will now scrap the text of the review.  Only the actually text of the review will be consiered.  This is indicated by the class = 'story-body-text story-content' under the paragraph tag <p>.  

```{r, eval = FALSE}
# For each URl in combo

library(RCurl)
library(XML)
library(indicoio)

#Set RCurl pars
curl <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt",  useragent = agent, followlocation = TRUE, curl=curl)

agent <- c(R.version$version.string, ",", R.version$platform)

movie_score <- data.frame()

for i in 1:length(combo$url){
  movie_url <-  combo$url[i] 
  # Get Review corresponding the url
  my_movie <- getURL(movie_url, curl = curl, verbose = TRUE)
  
  content_review = htmlTreeParse(my_movie, asText=TRUE, useInternalNodes = TRUE, encoding = 'UTF-8')
  
  plain.text <- xpathSApply(xmlRoot(content_review), "//p[@class = 'story-body-text story-content']", xmlValue)
  plain.text <- cat(paste(plain.text, collapse = "\n"))
  
  movie_score[i, 1] <- unlist(sentiment(as.character(rm_special(plaint.text)), 
                                         api_key = 'indicoio_API_Key'))
}
combo <- cbind(combo, movie_score)
```



```{r, include= FALSE, eval=FALSE}
url <- "https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/"
```

```{r, eval= FALSE}
data.url <- file(paste(url,"locations.csv", sep = ""), open="r" )
locations <- read.csv(data.url, sep=",", header=TRUE, stringsAsFactors = FALSE)
```

> The resulting CSV is available here:
https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/20160501nytreviews.csv


## Part 3. Peform Twitter Search and Twitter Sentiment Analysis

Once we established the list of available New York Times movie reviews, we ran the Twitter search using the movie title as a search term. To get better Twitter search results for one-word movies, we added the word "movie" to our search term.

This chunk retrieved the Twitter search results for a term. `searchTwitter` has some other settings that could probably improve results.

We used the indico package to perform sentiment analysis, resulting in a score of 0-100 for each tweet, 100 being positive.

We decided to use unique tweets only, therefore eliminating any retweet data.

For the benefit of our analysis, we filtered the Twitter search results to pull only the tweets which were coded with geographical location.

We chose the following cities at random: `r paste(locations$City, collapse = ", ")`

```{r, Searching Twitter, eval= FALSE}
library(indicoio)

rm_special <-function(x) iconv(x, "UTF-8", "UTF-8",sub='')

search_term <- function(term){
                  require(magrittr)
                  search_term <- gsub(" ", "+", term) %>%
                                 paste("+movie", sep = "")
                  return(search_term)
                }



df <- NULL
for (i in combo$display_title){
  SearchTerm <- search_term(i)
  for (j in seq(1:length(locations$City))){
      list <- suppressWarnings(searchTwitter(SearchTerm,  
                                             since = format(Sys.Date()-1,format="%Y-%m-%d"), 
                                             until = format(Sys.Date(),format="%Y-%m-%d"), n = 5000, 
                                    geocode = paste(locations$Lat[j], ",", locations$Long[j], ",30mi", sep = "" )))
          if (length(list) == 0){ 
            NULL
          } else { 
          tweetdf <- twListToDF(list)
          tweetdf <- as.data.frame(unique(tweetdf$text))
          colnames(tweetdf) <- c("tweet")
          tweetdf$city <- locations$City[j]
          tweetdf$movie <- as.character(i)
          tweetdf$day <- format(Sys.Date() ,format="%m.%d.%Y")
          df <- rbind(df,tweetdf)
          }
         } 
       }

df$Sentiment_Score <- unlist(sentiment(as.character(rm_special(df$tweet)), 
                                       api_key = '114ffc17022fd58af3da5cfd5f3e4388'))

write.csv(df, file = paste("Tweets for ", as.character(format(Sys.Date() ,format="%m.%d.%Y")), ".csv", sep = ""))

```

> The resulting CSV is available here: https://raw.githubusercontent.com/spsstudent15/Data607FinalProject/master/Tweets%20for%2005.03.2016.csv

## Part 4. Perform NYT Review Sentiment Analysis

For each New York Times review in the list, we pulled the text of the body of the review and ran it through sentiment analysis with indico, again resulting in a 0-100 score.

> The resulting CSV is available here:
[link to NYT review sentiment analysis]

# Data Transformation

We combined the Twitter sentiment and New York Times sentiment dataframes to view results.

# Statistical Analysis and Graphics

# New Features and Challenges

Twitter API / OAUTH

Choosing tweets

Testing different sentiment analysis machines

Indico package

Collaborating on Slack

Collaborating on Github 

# Insights and Conclusion
